# app_fastapi.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import asyncio
import aiofiles
import tempfile
import os
import json
import time
import logging
from typing import List, AsyncGenerator, Dict, Any
import math
from pydub import AudioSegment
from pydub.effects import normalize
import io
from openai import AsyncOpenAI
import anthropic
from dotenv import load_dotenv
import concurrent.futures
from functools import partial

# å˜—è©¦å°å…¥ backoffï¼Œå¦‚æœæ²’æœ‰å‰‡å®šç¾©ä¸€å€‹ç°¡å–®çš„é‡è©¦è£é£¾å™¨
try:
    import backoff
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£ backoff åº«ï¼Œå°‡ä½¿ç”¨ç°¡å–®é‡è©¦æ©Ÿåˆ¶")
    
    def backoff_decorator(*args, **kwargs):
        def decorator(func):
            return func
        return decorator
    
    class backoff:
        expo = backoff_decorator
        on_exception = backoff_decorator

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Social Work Report Generator",
    description="AI-powered social work report generation service",
    version="2.0.0"
)

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æª¢æŸ¥ API é‡‘é‘°
openai_api_key = os.getenv('OPENAI_API_KEY')
claude_api_key = os.getenv('CLAUDE_API_KEY')

if not openai_api_key:
    logger.error("âŒ æœªæ‰¾åˆ° OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise Exception("Missing OPENAI_API_KEY")

if not claude_api_key:
    logger.error("âŒ æœªæ‰¾åˆ° CLAUDE_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise Exception("Missing CLAUDE_API_KEY")

# API å®¢æˆ¶ç«¯
openai_client = AsyncOpenAI(api_key=openai_api_key)
claude_client = anthropic.Anthropic(api_key=claude_api_key)

# é…ç½®
MAX_CHUNK_SIZE = 15 * 1024 * 1024  # 15MB
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB ç¸½é™åˆ¶
SUPPORTED_FORMATS = {'mp3', 'mp4', 'm4a', 'wav', 'webm', 'ogg', 'flac', 'aac'}
MAX_CONCURRENT_TRANSCRIPTIONS = 2  # é™ä½ä¸¦ç™¼æ•¸
MAX_SEGMENT_MINUTES = 8  # æ¯æ®µæœ€å¤§åˆ†é˜æ•¸

class IntegratedAudioProcessor:
    """å®Œæ•´æ•´åˆçš„éŸ³é »è™•ç†å™¨"""
    
    # ==================== åŸºç¤éŸ³é »ä¿¡æ¯ç²å– ====================
    @staticmethod
    def _get_audio_info_sync(file_path: str) -> Dict[str, Any]:
        """åŒæ­¥ç²å–éŸ³é »æ–‡ä»¶ä¿¡æ¯"""
        try:
            audio = AudioSegment.from_file(file_path)
            return {
                'duration_ms': len(audio),
                'duration_min': len(audio) / 1000 / 60,
                'channels': audio.channels,
                'frame_rate': audio.frame_rate,
                'sample_width': audio.sample_width
            }
        except Exception as e:
            logger.error(f"pydub ç²å–éŸ³é »ä¿¡æ¯å¤±æ•—: {str(e)}")
            # å˜—è©¦ä½¿ç”¨ ffprobe ä½œç‚ºå‚™é¸
            return IntegratedAudioProcessor._get_audio_info_with_ffprobe(file_path)
    
    @staticmethod
    def _get_audio_info_with_ffprobe(file_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨ ffprobe ç²å–éŸ³é »ä¿¡æ¯ï¼ˆå‚™é¸æ–¹æ¡ˆï¼‰"""
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', '-show_streams', file_path
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                info = json.loads(result.stdout)
                duration = float(info.get('format', {}).get('duration', 0))
                streams = info.get('streams', [])
                audio_stream = next((s for s in streams if s.get('codec_type') == 'audio'), {})
                
                return {
                    'duration_ms': duration * 1000,
                    'duration_min': duration / 60,
                    'channels': int(audio_stream.get('channels', 1)),
                    'frame_rate': int(audio_stream.get('sample_rate', 44100)),
                    'sample_width': 2
                }
        except Exception as e:
            logger.error(f"ffprobe ç²å–éŸ³é »ä¿¡æ¯å¤±æ•—: {e}")
        
        return {}
    
    @staticmethod
    async def get_audio_info(file_path: str) -> Dict[str, Any]:
        """ç•°æ­¥ç²å–éŸ³é »æ–‡ä»¶ä¿¡æ¯"""
        try:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor, 
                    IntegratedAudioProcessor._get_audio_info_sync, 
                    file_path
                )
            return result
        except Exception as e:
            logger.error(f"ç²å–éŸ³é »ä¿¡æ¯å¤±æ•—: {str(e)}")
            return {}

    # ==================== éŸ³é »å£“ç¸® ====================
    @staticmethod
    def _compress_audio_sync(input_path: str, output_path: str, aggressive: bool = False) -> bool:
        """åŒæ­¥å£“ç¸®éŸ³é »æ–‡ä»¶"""
        try:
            logger.info(f"é–‹å§‹{'æ¿€é€²' if aggressive else 'æ¨™æº–'}å£“ç¸®: {input_path}")
            
            audio = AudioSegment.from_file(input_path)
            logger.info(f"åŸå§‹éŸ³é »: {len(audio)}ms, {audio.channels}è²é“, {audio.frame_rate}Hz")
            
            if aggressive:
                # æ¿€é€²å£“ç¸®è¨­ç½®
                audio = audio.set_frame_rate(12000)
                audio = audio.set_channels(1)
                audio = normalize(audio)
                
                audio.export(
                    output_path,
                    format="mp3",
                    bitrate="32k",
                    parameters=["-q:a", "9"]
                )
            else:
                # æ¨™æº–å£“ç¸®è¨­ç½®
                if audio.frame_rate > 16000:
                    audio = audio.set_frame_rate(16000)
                if audio.channels > 1:
                    audio = audio.set_channels(1)
                audio = normalize(audio)
                
                audio.export(
                    output_path,
                    format="mp3",
                    bitrate="64k",
                    parameters=["-q:a", "5"]
                )
            
            logger.info(f"éŸ³é »å£“ç¸®å®Œæˆ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"éŸ³é »å£“ç¸®å¤±æ•—: {str(e)}")
            return False

    @staticmethod
    async def compress_audio(input_path: str, output_path: str, aggressive: bool = False) -> bool:
        """ç•°æ­¥å£“ç¸®éŸ³é »æ–‡ä»¶"""
        try:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    IntegratedAudioProcessor._compress_audio_sync,
                    input_path,
                    output_path,
                    aggressive
                )
            return result
        except Exception as e:
            logger.error(f"ç•°æ­¥å£“ç¸®éŸ³é »å¤±æ•—: {str(e)}")
            return False

    # ==================== æ™ºèƒ½åˆ†å‰² ====================
    @staticmethod
    def should_split_by_duration(duration_minutes: float, file_size_mb: float) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦æŒ‰æ™‚é•·åˆ†å‰²"""
        if duration_minutes > 15:  # å¼·åˆ¶åˆ†å‰²
            return True
        elif duration_minutes > 10:  # å»ºè­°åˆ†å‰²
            return True
        elif duration_minutes > 5 and file_size_mb > 5:  # æ¢ä»¶åˆ†å‰²
            return True
        return False
    
    @staticmethod
    async def smart_split_audio(file_path: str, duration_minutes: float, max_segment_minutes: int = MAX_SEGMENT_MINUTES) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²éŸ³é » - æŒ‰æ™‚é•·åˆ†å‰²"""
        try:
            logger.info(f"ğŸ¯ æ™ºèƒ½åˆ†å‰²éŸ³é »: {file_path}, ç¸½æ™‚é•·: {duration_minutes:.1f} åˆ†é˜")
            
            # å¦‚æœä¸éœ€è¦åˆ†å‰²
            if not IntegratedAudioProcessor.should_split_by_duration(duration_minutes, os.path.getsize(file_path) / 1024 / 1024):
                logger.info("âœ… éŸ³é »æ™‚é•·é©ä¸­ï¼Œç„¡éœ€åˆ†å‰²")
                return [file_path]
            
            # è¨ˆç®—åˆ†æ®µæ•¸é‡
            num_segments = math.ceil(duration_minutes / max_segment_minutes)
            segment_duration_seconds = (duration_minutes * 60) / num_segments
            
            logger.info(f"ğŸ“Š å°‡åˆ†å‰²ç‚º {num_segments} æ®µï¼Œæ¯æ®µç´„ {segment_duration_seconds / 60:.1f} åˆ†é˜")
            
            def split_sync():
                audio = AudioSegment.from_file(file_path)
                total_duration_ms = len(audio)
                segment_duration_ms = total_duration_ms / num_segments
                
                segments = []
                for i in range(num_segments):
                    start_ms = int(i * segment_duration_ms)
                    end_ms = int((i + 1) * segment_duration_ms) if i < num_segments - 1 else total_duration_ms
                    
                    segment = audio[start_ms:end_ms]
                    
                    segment_path = f"{file_path}_time_segment_{i:02d}.mp3"
                    segment.export(
                        segment_path,
                        format="mp3",
                        bitrate="64k",
                        parameters=["-ar", "16000", "-ac", "1"]
                    )
                    
                    segments.append(segment_path)
                    segment_size = os.path.getsize(segment_path) / 1024 / 1024
                    segment_duration = len(segment) / 1000 / 60
                    
                    logger.info(f"âœ… åˆ†æ®µ {i}: {segment_duration:.1f} åˆ†é˜, {segment_size:.1f}MB")
                
                return segments
            
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                segments = await loop.run_in_executor(executor, split_sync)
            
            logger.info(f"ğŸ‰ æ™ºèƒ½åˆ†å‰²å®Œæˆï¼Œç”Ÿæˆ {len(segments)} å€‹åˆ†æ®µ")
            return segments
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½åˆ†å‰²å¤±æ•—: {e}")
            return [file_path]

    @staticmethod
    def _split_audio_by_size_sync(file_path: str, max_size: int) -> List[str]:
        """åŒæ­¥æŒ‰æ–‡ä»¶å¤§å°åˆ†å‰²éŸ³é »"""
        try:
            logger.info(f"é–‹å§‹æŒ‰å¤§å°åˆ†å‰²éŸ³é »: {file_path}, æœ€å¤§å¤§å°: {max_size / 1024 / 1024:.1f}MB")
            
            audio = AudioSegment.from_file(file_path)
            file_size = os.path.getsize(file_path)
            
            if file_size <= max_size:
                logger.info("æª”æ¡ˆå°æ–¼é™åˆ¶ï¼Œç„¡éœ€åˆ†å‰²")
                return [file_path]
            
            num_chunks = math.ceil(file_size / max_size)
            duration_per_chunk = len(audio) // num_chunks
            
            chunks = []
            for i in range(num_chunks):
                start_time = i * duration_per_chunk
                end_time = start_time + duration_per_chunk if i < num_chunks - 1 else len(audio)
                
                chunk = audio[start_time:end_time]
                chunk_path = f"{file_path}_chunk_{i}.mp3"
                chunk.export(
                    chunk_path, 
                    format="mp3", 
                    bitrate="64k",
                    parameters=["-q:a", "5"]
                )
                chunks.append(chunk_path)
                
                logger.info(f"åˆ†æ®µ {i} å·²ä¿å­˜: {chunk_path}")
            
            return chunks
            
        except Exception as e:
            logger.error(f"æŒ‰å¤§å°åˆ†å‰²å¤±æ•—: {str(e)}", exc_info=True)
            return [file_path]

    @staticmethod
    async def split_audio_by_size(file_path: str, max_size: int) -> List[str]:
        """ç•°æ­¥æŒ‰æ–‡ä»¶å¤§å°åˆ†å‰²éŸ³é »"""
        try:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    IntegratedAudioProcessor._split_audio_by_size_sync,
                    file_path,
                    max_size
                )
            return result
        except Exception as e:
            logger.error(f"ç•°æ­¥åˆ†å‰²éŸ³é »å¤±æ•—: {str(e)}")
            return [file_path]

    # ==================== è½‰æ›è™•ç† ====================
    @staticmethod
    async def simple_retry_transcribe(chunk_path: str, chunk_index: int, max_retries: int = 3) -> tuple:
        """ç°¡å–®é‡è©¦æ©Ÿåˆ¶çš„è½‰æ›"""
        last_error = None
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ¯ è½‰æ›åˆ†æ®µ {chunk_index} (ç¬¬ {attempt + 1} æ¬¡å˜—è©¦): {chunk_path}")
                
                if not os.path.exists(chunk_path):
                    raise FileNotFoundError(f"åˆ†æ®µæ–‡ä»¶ä¸å­˜åœ¨: {chunk_path}")
                
                file_size = os.path.getsize(chunk_path)
                logger.info(f"ğŸ“ åˆ†æ®µ {chunk_index} å¤§å°: {file_size / 1024 / 1024:.2f}MB")
                
                # å¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œå˜—è©¦é¡å¤–å£“ç¸®
                processing_path = chunk_path
                if file_size > 8 * 1024 * 1024:  # 8MB
                    logger.warning(f"âš ï¸ åˆ†æ®µ {chunk_index} éå¤§ï¼Œå˜—è©¦é¡å¤–å£“ç¸®...")
                    extra_compressed_path = f"{chunk_path}_extra_compressed.mp3"
                    
                    success = await IntegratedAudioProcessor.compress_audio(chunk_path, extra_compressed_path, aggressive=True)
                    if success and os.path.exists(extra_compressed_path):
                        new_size = os.path.getsize(extra_compressed_path)
                        if new_size < file_size and new_size > 0:
                            processing_path = extra_compressed_path
                            logger.info(f"âœ… é¡å¤–å£“ç¸®æˆåŠŸ: {new_size / 1024 / 1024:.2f}MB")
                
                # è®€å–ä¸¦ç™¼é€åˆ° OpenAI
                async with aiofiles.open(processing_path, 'rb') as audio_file:
                    audio_data = await audio_file.read()
                    
                    if not audio_data:
                        raise ValueError(f"åˆ†æ®µæ–‡ä»¶ç‚ºç©º: {processing_path}")
                    
                    audio_io = io.BytesIO(audio_data)
                    audio_io.name = f"chunk_{chunk_index}.mp3"
                    
                    logger.info(f"ğŸ“¡ ç™¼é€åˆ†æ®µ {chunk_index} åˆ° OpenAI...")
                    
                    response = await openai_client.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_io,
                        response_format="verbose_json",
                        language="zh",
                        timeout=120.0
                    )
                    
                    text = response.text
                    logger.info(f"âœ… åˆ†æ®µ {chunk_index} è½‰æ›æˆåŠŸï¼Œæ–‡å­—é•·åº¦: {len(text)}")
                    
                    # æ¸…ç†é¡å¤–å£“ç¸®æ–‡ä»¶
                    if processing_path != chunk_path:
                        try:
                            os.unlink(processing_path)
                        except:
                            pass
                    
                    return chunk_index, text, None
                    
            except Exception as e:
                last_error = e
                error_str = str(e)
                logger.error(f"âŒ åˆ†æ®µ {chunk_index} è½‰æ›å¤±æ•— (å˜—è©¦ {attempt + 1}): {error_str}")
                
                # æ ¹æ“šéŒ¯èª¤é¡å‹æ±ºå®šæ˜¯å¦é‡è©¦
                if "502" in error_str or "Bad Gateway" in error_str:
                    if attempt < max_retries - 1:
                        wait_time = 2 ** attempt  # æŒ‡æ•¸é€€é¿
                        logger.info(f"ğŸ”„ 502 éŒ¯èª¤ï¼Œ{wait_time} ç§’å¾Œé‡è©¦...")
                        await asyncio.sleep(wait_time)
                        continue
                elif "413" in error_str or "too large" in error_str.lower():
                    logger.error(f"ğŸ’¥ åˆ†æ®µ {chunk_index} æª”æ¡ˆéå¤§ï¼Œè·³éæ­¤åˆ†æ®µ")
                    return chunk_index, "[æª”æ¡ˆéå¤§ï¼Œç„¡æ³•è™•ç†]", f"æª”æ¡ˆéå¤§: {error_str}"
                elif "timeout" in error_str.lower():
                    if attempt < max_retries - 1:
                        logger.info(f"â° è«‹æ±‚è¶…æ™‚ï¼Œé‡è©¦...")
                        await asyncio.sleep(3)
                        continue
                
                # å…¶ä»–éŒ¯èª¤ï¼Œé‡è©¦ä¸€æ¬¡
                if attempt < max_retries - 1:
                    await asyncio.sleep(1)
                    continue
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
        error_msg = f"åˆ†æ®µ {chunk_index} æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—: {str(last_error)}"
        logger.error(error_msg)
        return chunk_index, "", error_msg

def send_sse_data(data_type: str, **kwargs) -> str:
    """ç™¼é€æ¨™æº–çš„ SSE æ ¼å¼è³‡æ–™"""
    data = {
        'type': data_type,
        'timestamp': time.time(),
        **kwargs
    }
    
    try:
        # ç¢ºä¿ JSON åºåˆ—åŒ–æ­£ç¢º
        json_str = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        
        # æ¨™æº– SSE æ ¼å¼ï¼šdata: + JSON + é›™æ›è¡Œç¬¦
        sse_message = f"data: {json_str}\n\n"
        
        # è¨˜éŒ„ç™¼é€çš„æ•¸æ“š (åƒ…åœ¨èª¿è©¦æ¨¡å¼)
        if data_type in ['progress', 'complete', 'error']:
            logger.debug(f"ğŸ“¤ ç™¼é€ SSE: {data_type} - {data.get('progress', 0)}% - {data.get('message', '')}")
        
        return sse_message
        
    except Exception as e:
        logger.error(f"âŒ SSE æ•¸æ“šåºåˆ—åŒ–å¤±æ•—: {e}")
        # è¿”å›éŒ¯èª¤æ¶ˆæ¯
        error_data = {
            'type': 'error',
            'error': f'æœå‹™å™¨å…§éƒ¨éŒ¯èª¤: {str(e)}',
            'timestamp': time.time()
        }
        return f"data: {json.dumps(error_data)}\n\n"

async def process_large_audio_smart(file_path: str) -> AsyncGenerator[str, None]:
    """æ™ºèƒ½éŸ³é »è™•ç† - å®Œæ•´ç‰ˆæœ¬"""
    temp_files = []
    
    try:
        logger.info(f"ğŸ¬ é–‹å§‹æ™ºèƒ½è™•ç†éŸ³é »æ–‡ä»¶: {file_path}")
        
        # 1. åŸºæœ¬æª¢æŸ¥
        file_size = os.path.getsize(file_path)
        yield send_sse_data('progress', progress=5, message=f'æª”æ¡ˆå¤§å°: {file_size / 1024 / 1024:.1f}MB')
        
        # 2. ç²å–éŸ³é »è©³ç´°ä¿¡æ¯
        audio_info = await IntegratedAudioProcessor.get_audio_info(file_path)
        duration_minutes = audio_info.get('duration_min', 0)
        
        if duration_minutes > 0:
            yield send_sse_data('progress', progress=10, 
                              message=f'éŸ³é »é•·åº¦: {duration_minutes:.1f} åˆ†é˜')
            
            if duration_minutes > 30:
                yield send_sse_data('progress', progress=12, 
                                  message='âš ï¸ éŸ³é »è¼ƒé•·ï¼Œå°‡æ¡ç”¨æ™ºèƒ½åˆ†å‰²ç­–ç•¥')
            elif duration_minutes > 15:
                yield send_sse_data('progress', progress=12, 
                                  message='ğŸ’¡ éŸ³é »ä¸­ç­‰é•·åº¦ï¼Œå°‡å„ªåŒ–è™•ç†')
        
        # 3. æ±ºå®šè™•ç†ç­–ç•¥
        processing_file = file_path
        
        # å¦‚æœéŸ³é »å¾ˆé•·ï¼Œå„ªå…ˆæŒ‰æ™‚é•·åˆ†å‰²
        if duration_minutes > 10:
            yield send_sse_data('progress', progress=15, message='æ¡ç”¨æ™‚é•·åˆ†å‰²ç­–ç•¥...')
            
            chunks = await IntegratedAudioProcessor.smart_split_audio(file_path, duration_minutes, MAX_SEGMENT_MINUTES)
            temp_files.extend([chunk for chunk in chunks if chunk != file_path])
            
            yield send_sse_data('progress', progress=30, 
                              message=f'æŒ‰æ™‚é•·åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} æ®µï¼ˆæ¯æ®µç´„ {MAX_SEGMENT_MINUTES} åˆ†é˜ï¼‰')
        
        # å¦‚æœæ–‡ä»¶å¾ˆå¤§ä½†æ™‚é•·ä¸é•·ï¼ŒæŒ‰å¤§å°åˆ†å‰²
        elif file_size > MAX_CHUNK_SIZE:
            yield send_sse_data('progress', progress=15, message='æª”æ¡ˆè¼ƒå¤§ï¼Œå…ˆå£“ç¸®...')
            
            compressed_path = f"{file_path}_compressed.mp3"
            temp_files.append(compressed_path)
            
            success = await IntegratedAudioProcessor.compress_audio(file_path, compressed_path)
            if success and os.path.exists(compressed_path):
                processing_file = compressed_path
                new_size = os.path.getsize(compressed_path) / 1024 / 1024
                yield send_sse_data('progress', progress=25, 
                                  message=f'å£“ç¸®å®Œæˆ: {new_size:.1f}MB')
            
            # æª¢æŸ¥å£“ç¸®å¾Œæ˜¯å¦é‚„éœ€è¦åˆ†å‰²
            if os.path.getsize(processing_file) > MAX_CHUNK_SIZE:
                chunks = await IntegratedAudioProcessor.split_audio_by_size(processing_file, MAX_CHUNK_SIZE)
                temp_files.extend([chunk for chunk in chunks if chunk != file_path and chunk != processing_file])
            else:
                chunks = [processing_file]
            
            yield send_sse_data('progress', progress=30, 
                              message=f'è™•ç†æº–å‚™å®Œæˆï¼Œå…± {len(chunks)} æ®µ')
        
        else:
            # å°æ–‡ä»¶ç›´æ¥è™•ç†
            chunks = [file_path]
            yield send_sse_data('progress', progress=30, message='æ–‡ä»¶å¤§å°é©ä¸­ï¼Œç›´æ¥è™•ç†')
        
        # 4. é©—è­‰åˆ†æ®µ
        valid_chunks = []
        for i, chunk_path in enumerate(chunks):
            if os.path.exists(chunk_path) and os.path.getsize(chunk_path) > 1024:
                chunk_size = os.path.getsize(chunk_path) / 1024 / 1024
                valid_chunks.append((chunk_path, i))
                logger.info(f"âœ“ æœ‰æ•ˆåˆ†æ®µ {i}: {chunk_size:.1f}MB")
            else:
                logger.error(f"âœ— ç„¡æ•ˆåˆ†æ®µ {i}: {chunk_path}")
        
        if not valid_chunks:
            raise ValueError("æ²’æœ‰æœ‰æ•ˆçš„éŸ³é »åˆ†æ®µ")
        
        # 5. æ™ºèƒ½è½‰æ›ç­–ç•¥
        yield send_sse_data('progress', progress=35, 
                          message=f'é–‹å§‹è½‰æ› {len(valid_chunks)} å€‹åˆ†æ®µ...')
        
        # æ ¹æ“šåˆ†æ®µæ•¸é‡èª¿æ•´ä¸¦ç™¼ç­–ç•¥
        if len(valid_chunks) > 5:
            batch_size = 1  # å¤§é‡åˆ†æ®µæ™‚ä¸²è¡Œè™•ç†
            delay_between_batches = 5
        elif len(valid_chunks) > 3:
            batch_size = 2
            delay_between_batches = 3
        else:
            batch_size = min(2, MAX_CONCURRENT_TRANSCRIPTIONS)
            delay_between_batches = 2
        
        logger.info(f"ğŸ¯ è½‰æ›ç­–ç•¥: æ‰¹æ¬¡å¤§å°={batch_size}, å»¶é²={delay_between_batches}ç§’")
        
        results = {}
        total_chunks = len(valid_chunks)
        processed_chunks = 0
        failed_chunks = 0
        
        for i in range(0, total_chunks, batch_size):
            batch_chunks = valid_chunks[i:i + batch_size]
            
            batch_info = f"ç¬¬ {i//batch_size + 1}/{math.ceil(total_chunks/batch_size)} æ‰¹"
            yield send_sse_data('progress', 
                              progress=35 + (processed_chunks / total_chunks) * 50,
                              message=f'è™•ç†{batch_info} ({len(batch_chunks)} æ®µ)...')
            
            # åŸ·è¡Œè½‰æ›
            tasks = [
                IntegratedAudioProcessor.simple_retry_transcribe(chunk_path, chunk_idx, max_retries=3)
                for chunk_path, chunk_idx in batch_chunks
            ]
            
            try:
                max_timeout = max(120, max(os.path.getsize(cp) for cp, _ in batch_chunks) / 1024 / 1024 * 30)
                
                batch_results = await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True),
                    timeout=max_timeout
                )
            except asyncio.TimeoutError:
                logger.error(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} è¶…æ™‚")
                batch_results = [Exception("è½‰æ›è¶…æ™‚") for _ in batch_chunks]
            
            # è™•ç†çµæœ
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"âŒ æ‰¹æ¬¡è™•ç†ç•°å¸¸: {result}")
                    failed_chunks += 1
                    continue
                    
                chunk_index, text, error = result
                if error:
                    logger.error(f"âŒ åˆ†æ®µ {chunk_index} éŒ¯èª¤: {error}")
                    failed_chunks += 1
                    if "æª”æ¡ˆéå¤§" not in error:
                        results[chunk_index] = ""  # è¨˜éŒ„å¤±æ•—ä½†ç¹¼çºŒ
                else:
                    results[chunk_index] = text
                    logger.info(f"âœ… åˆ†æ®µ {chunk_index} æˆåŠŸ: {len(text)} å­—")
                
                processed_chunks += 1
            
            # æ›´æ–°é€²åº¦
            progress = 35 + (processed_chunks / total_chunks) * 50
            success_rate = ((processed_chunks - failed_chunks) / processed_chunks * 100) if processed_chunks > 0 else 0
            yield send_sse_data('progress', progress=int(progress), 
                              message=f'å·²å®Œæˆ {processed_chunks}/{total_chunks} æ®µ (æˆåŠŸç‡: {success_rate:.0f}%)')
            
            # æ‰¹æ¬¡é–“å»¶é²
            if i + batch_size < total_chunks:
                yield send_sse_data('progress', progress=int(progress), 
                                  message=f'æš«åœ {delay_between_batches} ç§’ï¼Œé¿å… API é™åˆ¶...')
                await asyncio.sleep(delay_between_batches)
        
        # 6. åˆä½µçµæœ
        yield send_sse_data('progress', progress=90, message='åˆä½µè½‰æ›çµæœ...')
        
        final_transcript = ""
        successful_chunks = processed_chunks - failed_chunks
        
        # æŒ‰é †åºåˆä½µ
        for i in range(len(chunks)):
            if i in results and results[i]:
                final_transcript += results[i] + " "
        
        if successful_chunks == 0:
            raise Exception(f"æ‰€æœ‰ {total_chunks} å€‹åˆ†æ®µéƒ½è½‰æ›å¤±æ•—")
        
        final_transcript = final_transcript.strip()
        final_transcript = " ".join(final_transcript.split())
        
        success_rate = (successful_chunks / total_chunks) * 100
        
        # 7. ä¸²æµç™¼é€çµæœ
        sentences = []
        for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
            final_transcript = final_transcript.replace(delimiter, delimiter + '|')
        
        sentences = [s.strip() for s in final_transcript.split('|') if s.strip()]
        
        yield send_sse_data('progress', progress=95, 
                          message=f'é–‹å§‹ç™¼é€çµæœ... (å…± {len(sentences)} å¥)')
        
        for i, sentence in enumerate(sentences):
            if sentence:
                if not sentence.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                    sentence += 'ã€‚'
                
                yield send_sse_data('chunk', 
                                  text=sentence,
                                  progress=95 + (i / len(sentences)) * 5)
                await asyncio.sleep(0.03)
        
        # 8. å®Œæˆ
        yield send_sse_data('complete', 
                          progress=100, 
                          message=f'è™•ç†å®Œæˆï¼å…± {len(final_transcript)} å­— (æˆåŠŸç‡: {success_rate:.1f}%)',
                          stats={
                              'original_duration_minutes': duration_minutes,
                              'total_chunks': total_chunks,
                              'successful_chunks': successful_chunks,
                              'failed_chunks': failed_chunks,
                              'success_rate': success_rate,
                              'total_characters': len(final_transcript),
                              'total_sentences': len(sentences),
                              'processing_strategy': 'time_based' if duration_minutes > 10 else 'size_based'
                          })
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½éŸ³é »è™•ç†å¤±æ•—: {str(e)}", exc_info=True)
        yield send_sse_data('error', 
                          error=f'è™•ç†å¤±æ•—: {str(e)}',
                          error_type=type(e).__name__)
    
    finally:
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        for temp_file in temp_files:
            try:
                if temp_file and os.path.exists(temp_file):
                    os.unlink(temp_file)
                    logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤: {os.path.basename(temp_file)}")
            except Exception as e:
                logger.warning(f"æ¸…ç†å¤±æ•—: {e}")

@app.post("/backend/transcribe")
async def transcribe_audio_smart(audio: UploadFile = File(...)):
    """æ™ºèƒ½éŸ³é »è½‰æ–‡å­— API - æ ¹æ“šæ–‡ä»¶ç‰¹æ€§è‡ªå‹•é¸æ“‡æœ€ä½³è™•ç†ç­–ç•¥"""
    
    # æª¢æŸ¥æ–‡ä»¶å
    if not audio.filename:
        raise HTTPException(status_code=400, detail="æ²’æœ‰æä¾›æª”æ¡ˆåç¨±")
    
    # æª¢æŸ¥æ–‡ä»¶æ ¼å¼
    file_extension = audio.filename.split('.')[-1].lower()
    if file_extension not in SUPPORTED_FORMATS:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚æ”¯æ´æ ¼å¼: {', '.join(SUPPORTED_FORMATS)}"
        )
    
    # è®€å–æ–‡ä»¶å…§å®¹ä¸¦æª¢æŸ¥å¤§å°
    content = await audio.read()
    file_size = len(content)
    
    logger.info(f"ğŸ“ æ”¶åˆ°éŸ³é »æ–‡ä»¶: {audio.filename}, å¤§å°: {file_size / 1024 / 1024:.2f}MB")
    
    # æª¢æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
    if file_size > MAX_FILE_SIZE:
        logger.warning(f"âŒ æ–‡ä»¶éå¤§: {file_size / 1024 / 1024:.2f}MB > {MAX_FILE_SIZE / 1024 / 1024}MB")
        raise HTTPException(
            status_code=413,
            detail=f"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ ({MAX_FILE_SIZE // (1024*1024)}MB)"
        )
    
    if file_size == 0:
        logger.error("âŒ æ–‡ä»¶ç‚ºç©º")
        raise HTTPException(status_code=400, detail="ä¸Šå‚³çš„æ–‡ä»¶ç‚ºç©º")
    
    # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as tmp_file:
        tmp_file.write(content)
        temp_file_path = tmp_file.name
    
    logger.info(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {temp_file_path}")
    
    async def generate_response():
        try:
            yield send_sse_data('progress', progress=0, message='æ­£åœ¨åˆ†æéŸ³é »æ–‡ä»¶...')
            
            # ä½¿ç”¨æ™ºèƒ½è™•ç†å‡½æ•¸
            async for chunk in process_large_audio_smart(temp_file_path):
                yield chunk
                
        except Exception as e:
            logger.error(f"è™•ç†éŒ¯èª¤: {str(e)}", exc_info=True)
            
            # æä¾›æ›´è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯
            error_message = str(e)
            if "502" in error_message or "Bad Gateway" in error_message:
                error_message = "OpenAI æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦"
            elif "timeout" in error_message.lower():
                error_message = "è«‹æ±‚è¶…æ™‚ï¼Œæª”æ¡ˆå¯èƒ½éå¤§ï¼Œå»ºè­°åˆ†æ®µä¸Šå‚³"
            elif "413" in error_message or "too large" in error_message.lower():
                error_message = "æª”æ¡ˆéå¤§ï¼Œè«‹å£“ç¸®å¾Œå†è©¦æˆ–åˆ†æ®µä¸Šå‚³"
            
            yield send_sse_data('error', error=error_message)
            
        finally:
            try:
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                    logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†åŸå§‹æ–‡ä»¶: {temp_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ä¸Šå‚³æ–‡ä»¶å¤±æ•—: {e}")
    
    return StreamingResponse(
        generate_response(),
        media_type='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Content-Type': 'text/event-stream; charset=utf-8',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Cache-Control',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'X-Accel-Buffering': 'no'
        }
    )

def load_prompt_templates(filename):
    """è¼‰å…¥ prompt æ¨¡æ¿"""
    try:
        with open(f'prompts/{filename}', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"æ‰¾ä¸åˆ° prompts/{filename} æª”æ¡ˆ")
        return None
    except json.JSONDecodeError:
        logger.error(f"prompts/{filename}' æ ¼å¼éŒ¯èª¤")
        return None
    
def build_report_prompt(transcript, social_worker_notes, selected_sections, required_sections):
    """å»ºæ§‹è¨˜éŒ„ç”Ÿæˆçš„ prompt"""
    templates = load_prompt_templates(filename="report_prompts.json")
    if not templates:
        raise Exception("ç„¡æ³•è¼‰å…¥ prompt æ¨¡æ¿")
    
    # åŸºæœ¬æ¨¡æ¿
    base_template = templates['report_generation']['base_template']
    
    # å¯é¸æ®µè½çš„é¡å¤–æŒ‡ç¤º
    optional_instructions = templates['report_generation']['optional_sections']
    
    # å»ºæ§‹é¡å¤–æŒ‡ç¤ºæ–‡å­—
    additional_instructions = []
    
    for section in selected_sections:
        if section in optional_instructions:
            instruction = optional_instructions[section]
            additional_instructions.append(f"\n{instruction['title']}\n{instruction['content']}")
    
    # çµ„åˆå®Œæ•´çš„ prompt
    full_prompt = base_template
    
    if additional_instructions:
        full_prompt += "\n\né¡å¤–è©•ä¼°é …ç›®ï¼š" + "".join(additional_instructions)
    
    # åŠ å…¥é€å­—ç¨¿å’Œç¤¾å·¥è£œå……èªªæ˜
    input_content = f"é€å­—ç¨¿å…§å®¹ï¼š\n{transcript}"
    if social_worker_notes and social_worker_notes.strip():
        input_content += f"\n\nç¤¾å·¥è£œå……èªªæ˜ï¼š\n{social_worker_notes}"
    
    # æ›¿æ› input è®Šæ•¸
    full_prompt = full_prompt.replace('{input}', input_content)
    
    return full_prompt


# ä¿æŒå…¶ä»– API ç«¯é» (è¨˜éŒ„ç”Ÿæˆã€è™•é‡è¨ˆç•«ç­‰)
async def generate_report_streaming(transcript: str, social_worker_notes: str, 
                                  selected_sections: List[str], required_sections: List[str]):
    """ç”Ÿæˆè¨˜éŒ„ (ä¿æŒåŸæœ‰é‚è¼¯)"""
    try:
        yield send_sse_data('progress', progress=10, message='æº–å‚™ç”Ÿæˆè¨˜éŒ„...')
        
        # ç°¡åŒ–çš„ prompt (ä½ å¯ä»¥æ ¹æ“šéœ€è¦ä¿®æ”¹)
        prompt = build_report_prompt(transcript, social_worker_notes, selected_sections, required_sections)
        
        yield send_sse_data('progress', progress=20, message='æ­£åœ¨ç”Ÿæˆè¨˜éŒ„...')
        
        # èª¿ç”¨ Claude API
        with claude_client.messages.stream(
            model="claude-3-sonnet-20240229",
            max_tokens=4000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        ) as stream:
            
            current_progress = 30
            for event in stream:
                if event.type == "content_block_delta":
                    text_chunk = event.delta.text
                    yield send_sse_data('chunk', text=text_chunk, progress=min(95, current_progress))
                    current_progress += 0.5
                elif event.type == "message_stop":
                    yield send_sse_data('complete', progress=100, message='è¨˜éŒ„ç”Ÿæˆå®Œæˆ')
                    break
                    
    except Exception as e:
        logger.error(f"è¨˜éŒ„ç”Ÿæˆå¤±æ•—: {str(e)}")
        yield send_sse_data('error', error=f'è¨˜éŒ„ç”Ÿæˆå¤±æ•—: {str(e)}')

@app.post("/backend/generate-report")
async def generate_report(request: Request):
    """ç”Ÿæˆè¨˜éŒ„åˆç¨¿ API"""
    try:
        data = await request.json()
        
        transcript = data.get('transcript', '').strip()
        if not transcript:
            raise HTTPException(status_code=400, detail="é€å­—ç¨¿å…§å®¹ä¸èƒ½ç‚ºç©º")
        
        social_worker_notes = data.get('socialWorkerNotes', '').strip()
        selected_sections = data.get('selectedSections', [])
        required_sections = data.get('requiredSections', [])
        
        async def generate_response():
            async for chunk in generate_report_streaming(transcript, social_worker_notes, selected_sections, required_sections):
                yield chunk
        
        return StreamingResponse(
            generate_response(),
            media_type='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
            }
        )
        
    except Exception as e:
        logger.error(f"API è™•ç†éŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨éŒ¯èª¤: {str(e)}")

@app.get("/backend/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        'status': 'healthy',
        'service': 'social-work-report-generator-fastapi',
        'timestamp': time.time(),
        'version': '2.0.0',
        'apis': ['transcribe', 'generate-report', 'generate-treatment-plan'],
        'features': ['large_file_support', 'audio_compression', 'concurrent_processing']
    }

@app.get("/backend/supported-formats")
async def get_supported_formats():
    """ç²å–æ”¯æ´çš„æ ¼å¼å’Œé…ç½®"""
    return {
        'supported_formats': list(SUPPORTED_FORMATS),
        'max_file_size_mb': MAX_FILE_SIZE // (1024*1024),
        'max_chunk_size_mb': MAX_CHUNK_SIZE // (1024*1024),
        'max_concurrent_transcriptions': MAX_CONCURRENT_TRANSCRIPTIONS
    }

if __name__ == "__main__":
    import uvicorn
    
    logger.info("ğŸš€ å•Ÿå‹• FastAPI ç¤¾å·¥å ±å‘Šç”Ÿæˆæœå‹™...")
    logger.info(f"âœ… OpenAI API é‡‘é‘°å·²è¼‰å…¥")
    logger.info(f"âœ… Claude API é‡‘é‘°å·²è¼‰å…¥")
    
    # ä¿®æ­£çš„å•Ÿå‹•æ–¹å¼
    uvicorn.run(
        "app_fastapi:app",  # ä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼
        host="0.0.0.0", 
        port=5174,
        reload=True,        # ç¾åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ reload
        log_level="info",
        access_log=True     # é¡¯ç¤ºè¨ªå•æ—¥èªŒ
    )