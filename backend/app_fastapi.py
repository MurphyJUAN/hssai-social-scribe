# app_fastapi.py
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import asyncio
import aiofiles
import tempfile
import os
import json
import time
import logging
from typing import List, AsyncGenerator, Dict, Any
import math
from pydub import AudioSegment
from pydub.effects import normalize
import io
from openai import AsyncOpenAI
import anthropic
from dotenv import load_dotenv
import concurrent.futures
from functools import partial

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Social Work Report Generator",
    description="AI-powered social work report generation service",
    version="2.0.0"
)

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æª¢æŸ¥ API é‡‘é‘°
openai_api_key = os.getenv('OPENAI_API_KEY')
claude_api_key = os.getenv('CLAUDE_API_KEY')

if not openai_api_key:
    logger.error("âŒ æœªæ‰¾åˆ° OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise Exception("Missing OPENAI_API_KEY")

if not claude_api_key:
    logger.error("âŒ æœªæ‰¾åˆ° CLAUDE_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise Exception("Missing CLAUDE_API_KEY")

# API å®¢æˆ¶ç«¯
openai_client = AsyncOpenAI(api_key=openai_api_key)
claude_client = anthropic.Anthropic(api_key=claude_api_key)

# é…ç½®
MAX_CHUNK_SIZE = 15 * 1024 * 1024  # 15MB
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB ç¸½é™åˆ¶
SUPPORTED_FORMATS = {'mp3', 'mp4', 'm4a', 'wav', 'webm', 'ogg', 'flac', 'aac'}
MAX_CONCURRENT_TRANSCRIPTIONS = 3  # OpenAI API ä¸¦ç™¼é™åˆ¶

class AudioProcessor:
    """ä¿®å¾©çš„éŸ³é »è™•ç†é¡"""
    
    @staticmethod
    def _get_audio_info_sync(file_path: str) -> Dict[str, Any]:
        """åŒæ­¥ç²å–éŸ³é »æ–‡ä»¶ä¿¡æ¯"""
        try:
            from pydub import AudioSegment
            audio = AudioSegment.from_file(file_path)
            
            return {
                'duration_ms': len(audio),
                'duration_min': len(audio) / 1000 / 60,
                'channels': audio.channels,
                'frame_rate': audio.frame_rate,
                'sample_width': audio.sample_width
            }
        except Exception as e:
            logger.error(f"åŒæ­¥ç²å–éŸ³é »ä¿¡æ¯å¤±æ•—: {str(e)}")
            return {}
    
    @staticmethod
    async def get_audio_info(file_path: str) -> Dict[str, Any]:
        """ç•°æ­¥ç²å–éŸ³é »æ–‡ä»¶ä¿¡æ¯"""
        try:
            # åœ¨ç·šç¨‹æ± ä¸­é‹è¡ŒåŒæ­¥å‡½æ•¸
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor, 
                    AudioProcessor._get_audio_info_sync, 
                    file_path
                )
            return result
        except Exception as e:
            logger.error(f"ç²å–éŸ³é »ä¿¡æ¯å¤±æ•—: {str(e)}")
            return {}

    @staticmethod
    def _compress_audio_sync(input_path: str, output_path: str) -> bool:
        """åŒæ­¥å£“ç¸®éŸ³é »æ–‡ä»¶"""
        try:
            from pydub import AudioSegment
            from pydub.effects import normalize
            
            logger.info(f"é–‹å§‹å£“ç¸®éŸ³é »: {input_path}")
            
            # è¼‰å…¥éŸ³é »
            audio = AudioSegment.from_file(input_path)
            logger.info(f"åŸå§‹éŸ³é »: {len(audio)}ms, {audio.channels}è²é“, {audio.frame_rate}Hz")
            
            # å£“ç¸®è¨­ç½®ï¼šé‡å°èªéŸ³è­˜åˆ¥å„ªåŒ–
            # 1. é™ä½æ¡æ¨£ç‡åˆ° 16kHz (èªéŸ³è­˜åˆ¥æœ€ä½³)
            if audio.frame_rate > 16000:
                audio = audio.set_frame_rate(16000)
            
            # 2. è½‰æ›ç‚ºå–®è²é“
            if audio.channels > 1:
                audio = audio.set_channels(1)
            
            # 3. æ­£è¦åŒ–éŸ³é‡
            audio = normalize(audio)
            
            # å°å‡ºç‚ºå£“ç¸®æ ¼å¼
            audio.export(
                output_path,
                format="mp3",
                bitrate="64k",
                parameters=["-q:a", "5"]  # ä¸­ç­‰å£“ç¸®ç‡ï¼Œä¿æŒå“è³ª
            )
            
            logger.info(f"éŸ³é »å£“ç¸®å®Œæˆ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"åŒæ­¥å£“ç¸®éŸ³é »å¤±æ•—: {str(e)}")
            return False

    @staticmethod
    async def compress_audio(input_path: str, output_path: str) -> bool:
        """ç•°æ­¥å£“ç¸®éŸ³é »æ–‡ä»¶"""
        try:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    AudioProcessor._compress_audio_sync,
                    input_path,
                    output_path
                )
            return result
        except Exception as e:
            logger.error(f"ç•°æ­¥å£“ç¸®éŸ³é »å¤±æ•—: {str(e)}")
            return False

    @staticmethod
    def _split_audio_sync(file_path: str, max_size: int) -> List[str]:
        """åŒæ­¥åˆ†å‰²éŸ³é »"""
        try:
            from pydub import AudioSegment
            
            logger.info(f"é–‹å§‹åˆ†å‰²éŸ³é »: {file_path}, æœ€å¤§å¤§å°: {max_size / 1024 / 1024:.1f}MB")
            
            # è¼‰å…¥éŸ³é »
            audio = AudioSegment.from_file(file_path)
            
            # è¨ˆç®—éœ€è¦åˆ†å‰²çš„æ®µæ•¸
            file_size = os.path.getsize(file_path)
            logger.info(f"æª”æ¡ˆå¤§å°: {file_size / 1024 / 1024:.1f}MB")
            
            if file_size <= max_size:
                logger.info("æª”æ¡ˆå°æ–¼é™åˆ¶ï¼Œç„¡éœ€åˆ†å‰²")
                return [file_path]
            
            num_chunks = math.ceil(file_size / max_size)
            logger.info(f"éœ€è¦åˆ†å‰²ç‚º {num_chunks} æ®µ")
            
            # è¨ˆç®—æ¯æ®µçš„æ™‚é•· (æ¯«ç§’)
            duration_per_chunk = len(audio) // num_chunks
            
            chunks = []
            for i in range(num_chunks):
                start_time = i * duration_per_chunk
                end_time = start_time + duration_per_chunk if i < num_chunks - 1 else len(audio)
                
                logger.info(f"å‰µå»ºåˆ†æ®µ {i}: {start_time}ms - {end_time}ms")
                
                # æå–éŸ³é »æ®µ
                chunk = audio[start_time:end_time]
                
                # ä¿å­˜åˆ†æ®µ
                chunk_path = f"{file_path}_chunk_{i}.mp3"
                chunk.export(
                    chunk_path, 
                    format="mp3", 
                    bitrate="64k",
                    parameters=["-q:a", "5"]
                )
                chunks.append(chunk_path)
                
                logger.info(f"åˆ†æ®µ {i} å·²ä¿å­˜: {chunk_path}")
            
            return chunks
            
        except Exception as e:
            logger.error(f"åŒæ­¥åˆ†å‰²éŸ³é »å¤±æ•—: {str(e)}", exc_info=True)
            return [file_path]  # è¿”å›åŸæ–‡ä»¶

    @staticmethod
    async def split_audio_by_size(file_path: str, max_size: int) -> List[str]:
        """ç•°æ­¥æŒ‰æ–‡ä»¶å¤§å°åˆ†å‰²éŸ³é »"""
        try:
            loop = asyncio.get_event_loop()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                result = await loop.run_in_executor(
                    executor,
                    AudioProcessor._split_audio_sync,
                    file_path,
                    max_size
                )
            return result
        except Exception as e:
            logger.error(f"ç•°æ­¥åˆ†å‰²éŸ³é »å¤±æ•—: {str(e)}")
            return [file_path]

    @staticmethod
    async def transcribe_chunk(chunk_path: str, chunk_index: int) -> tuple:
        """è½‰æ›å–®å€‹éŸ³é »åˆ†æ®µ"""
        try:
            logger.info(f"é–‹å§‹è½‰æ›åˆ†æ®µ {chunk_index}: {chunk_path}")
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(chunk_path):
                raise FileNotFoundError(f"åˆ†æ®µæ–‡ä»¶ä¸å­˜åœ¨: {chunk_path}")
            
            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(chunk_path)
            logger.info(f"åˆ†æ®µ {chunk_index} å¤§å°: {file_size / 1024 / 1024:.2f}MB")
            
            # è®€å–æ–‡ä»¶
            async with aiofiles.open(chunk_path, 'rb') as audio_file:
                audio_data = await audio_file.read()
                
                if not audio_data:
                    raise ValueError(f"åˆ†æ®µæ–‡ä»¶ç‚ºç©º: {chunk_path}")
                
                # å‰µå»º BytesIO å°è±¡
                audio_io = io.BytesIO(audio_data)
                audio_io.name = f"chunk_{chunk_index}.mp3"
                
                logger.info(f"ç™¼é€åˆ†æ®µ {chunk_index} åˆ° OpenAI...")
                
                # èª¿ç”¨ OpenAI API
                response = await openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_io,
                    response_format="verbose_json",
                    language="zh"
                )
                
                text = response.text
                logger.info(f"åˆ†æ®µ {chunk_index} è½‰æ›å®Œæˆï¼Œæ–‡å­—é•·åº¦: {len(text)}")
                
                return chunk_index, text, None
                
        except Exception as e:
            error_msg = f"åˆ†æ®µ {chunk_index} è½‰æ›å¤±æ•—: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return chunk_index, "", error_msg

def send_sse_data(data_type: str, **kwargs) -> str:
    """ç™¼é€æ¨™æº–çš„ SSE æ ¼å¼è³‡æ–™"""
    data = {
        'type': data_type,
        'timestamp': time.time(),
        **kwargs
    }
    
    try:
        # ç¢ºä¿ JSON åºåˆ—åŒ–æ­£ç¢º
        json_str = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        
        # æ¨™æº– SSE æ ¼å¼ï¼šdata: + JSON + é›™æ›è¡Œç¬¦
        sse_message = f"data: {json_str}\n\n"
        
        # è¨˜éŒ„ç™¼é€çš„æ•¸æ“š (åƒ…åœ¨èª¿è©¦æ¨¡å¼)
        if data_type in ['progress', 'complete', 'error']:
            logger.debug(f"ğŸ“¤ ç™¼é€ SSE: {data_type} - {data.get('progress', 0)}% - {data.get('message', '')}")
        
        return sse_message
        
    except Exception as e:
        logger.error(f"âŒ SSE æ•¸æ“šåºåˆ—åŒ–å¤±æ•—: {e}")
        # è¿”å›éŒ¯èª¤æ¶ˆæ¯
        error_data = {
            'type': 'error',
            'error': f'æœå‹™å™¨å…§éƒ¨éŒ¯èª¤: {str(e)}',
            'timestamp': time.time()
        }
        return f"data: {json.dumps(error_data)}\n\n"

async def process_large_audio(file_path: str) -> AsyncGenerator[str, None]:
    """è™•ç†å¤§å‹éŸ³é »æ–‡ä»¶çš„ä¸»è¦å‡½æ•¸ (ä¿®å¾©ç‰ˆ)"""
    temp_files = []
    
    try:
        logger.info(f"é–‹å§‹è™•ç†éŸ³é »æ–‡ä»¶: {file_path}")
        
        # 1. æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"éŸ³é »æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # 2. æª¢æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            raise ValueError("éŸ³é »æ–‡ä»¶ç‚ºç©º")
        
        yield send_sse_data('progress', 
                          progress=5, 
                          message=f'æª”æ¡ˆå¤§å°: {file_size / 1024 / 1024:.1f}MB')
        
        # 3. ç²å–éŸ³é »ä¿¡æ¯ (å¯é¸ï¼Œå¤±æ•—ä¸å½±éŸ¿ä¸»æµç¨‹)
        try:
            audio_info = await AudioProcessor.get_audio_info(file_path)
            if audio_info:
                duration_min = audio_info.get('duration_min', 0)
                yield send_sse_data('progress', 
                                  progress=8, 
                                  message=f'éŸ³é »é•·åº¦: {duration_min:.1f} åˆ†é˜')
        except Exception as e:
            logger.warning(f"ç²å–éŸ³é »ä¿¡æ¯å¤±æ•—ï¼Œç¹¼çºŒè™•ç†: {e}")
        
        # 4. å¦‚æœæ–‡ä»¶è¼ƒå¤§ï¼Œå…ˆå£“ç¸®
        processing_file = file_path
        if file_size > MAX_CHUNK_SIZE:
            yield send_sse_data('progress', progress=10, message='æª”æ¡ˆè¼ƒå¤§ï¼Œé–‹å§‹å£“ç¸®...')
            
            compressed_path = f"{file_path}_compressed.mp3"
            temp_files.append(compressed_path)
            
            success = await AudioProcessor.compress_audio(file_path, compressed_path)
            if success and os.path.exists(compressed_path):
                new_size = os.path.getsize(compressed_path)
                if new_size > 0:  # ç¢ºä¿å£“ç¸®å¾Œçš„æ–‡ä»¶ä¸ç‚ºç©º
                    processing_file = compressed_path
                    compression_ratio = (1 - new_size / file_size) * 100
                    yield send_sse_data('progress', progress=20, 
                                      message=f'å£“ç¸®å®Œæˆï¼Œå¤§å°: {new_size / 1024 / 1024:.1f}MB (å£“ç¸® {compression_ratio:.1f}%)')
                else:
                    logger.warning("å£“ç¸®å¾Œæ–‡ä»¶ç‚ºç©ºï¼Œä½¿ç”¨åŸæ–‡ä»¶")
                    yield send_sse_data('progress', progress=20, message='å£“ç¸®å¾Œæ–‡ä»¶ç•°å¸¸ï¼Œä½¿ç”¨åŸæ–‡ä»¶')
            else:
                logger.warning("å£“ç¸®å¤±æ•—ï¼Œä½¿ç”¨åŸæ–‡ä»¶")
                yield send_sse_data('progress', progress=20, message='å£“ç¸®å¤±æ•—ï¼Œä½¿ç”¨åŸæ–‡ä»¶')
        
        # 5. åˆ†å‰²éŸ³é »
        yield send_sse_data('progress', progress=25, message='æº–å‚™åˆ†å‰²éŸ³é »...')
        chunks = await AudioProcessor.split_audio_by_size(processing_file, MAX_CHUNK_SIZE)
        
        # æ¸…ç†åˆ†æ®µæ–‡ä»¶åˆ—è¡¨ï¼Œæ’é™¤åŸæ–‡ä»¶
        chunk_temp_files = [chunk for chunk in chunks if chunk != file_path and chunk != processing_file]
        temp_files.extend(chunk_temp_files)
        
        if len(chunks) > 1:
            yield send_sse_data('progress', progress=30, 
                              message=f'éŸ³é »å·²åˆ†å‰²ç‚º {len(chunks)} æ®µ')
        else:
            yield send_sse_data('progress', progress=30, message='éŸ³é »ç„¡éœ€åˆ†å‰²')
        
        # 6. é©—è­‰æ‰€æœ‰åˆ†æ®µæ–‡ä»¶
        valid_chunks = []
        for i, chunk_path in enumerate(chunks):
            if os.path.exists(chunk_path) and os.path.getsize(chunk_path) > 0:
                valid_chunks.append((chunk_path, i))
            else:
                logger.error(f"åˆ†æ®µæ–‡ä»¶ç„¡æ•ˆ: {chunk_path}")
        
        if not valid_chunks:
            raise ValueError("æ²’æœ‰æœ‰æ•ˆçš„éŸ³é »åˆ†æ®µ")
        
        # 7. ä¸¦ç™¼è½‰æ›æ‰€æœ‰æœ‰æ•ˆåˆ†æ®µ
        yield send_sse_data('progress', progress=35, message='é–‹å§‹è½‰æ›éŸ³é »ç‚ºæ–‡å­—...')
        
        batch_size = MAX_CONCURRENT_TRANSCRIPTIONS
        results = {}
        total_chunks = len(valid_chunks)
        processed_chunks = 0
        
        for i in range(0, total_chunks, batch_size):
            batch_chunks = valid_chunks[i:i + batch_size]
            
            # å‰µå»ºç•¶å‰æ‰¹æ¬¡çš„ä»»å‹™
            tasks = [
                AudioProcessor.transcribe_chunk(chunk_path, chunk_idx) 
                for chunk_path, chunk_idx in batch_chunks
            ]
            
            yield send_sse_data('progress', 
                              progress=35 + (processed_chunks / total_chunks) * 50,
                              message=f'è™•ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼Œå…± {len(batch_chunks)} æ®µ')
            
            # ä¸¦ç™¼åŸ·è¡Œç•¶å‰æ‰¹æ¬¡
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è™•ç†æ‰¹æ¬¡çµæœ
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"æ‰¹æ¬¡è™•ç†ç•°å¸¸: {result}")
                    continue
                    
                chunk_index, text, error = result
                if error:
                    logger.error(f"åˆ†æ®µè™•ç†éŒ¯èª¤: {error}")
                    results[chunk_index] = f"[åˆ†æ®µ {chunk_index} è™•ç†å¤±æ•—]"
                else:
                    results[chunk_index] = text
                
                processed_chunks += 1
                progress = 35 + (processed_chunks / total_chunks) * 50
                yield send_sse_data('progress', progress=int(progress), 
                                  message=f'å·²å®Œæˆ {processed_chunks}/{total_chunks} æ®µ')
            
            # æ‰¹æ¬¡é–“å»¶é²
            if i + batch_size < total_chunks:
                await asyncio.sleep(1)
        
        # 8. åˆä½µçµæœ
        yield send_sse_data('progress', progress=90, message='åˆä½µè½‰æ›çµæœ...')
        
        final_transcript = ""
        successful_chunks = 0
        
        # æŒ‰é †åºåˆä½µçµæœ
        for i in range(len(chunks)):
            if i in results and results[i] and not results[i].startswith("[åˆ†æ®µ"):
                final_transcript += results[i] + " "
                successful_chunks += 1
        
        if successful_chunks == 0:
            raise Exception("æ‰€æœ‰éŸ³é »åˆ†æ®µè½‰æ›éƒ½å¤±æ•—äº†")
        
        # 9. æ¸…ç†ä¸¦æ ¼å¼åŒ–æ–‡å­—
        final_transcript = final_transcript.strip()
        final_transcript = " ".join(final_transcript.split())
        
        yield send_sse_data('progress', progress=95, 
                          message=f'è½‰æ›å®Œæˆï¼ŒæˆåŠŸè™•ç† {successful_chunks}/{len(chunks)} æ®µ')
        
        # 10. æ¨¡æ“¬ä¸²æµç™¼é€æ–‡å­—
        sentences = []
        for delimiter in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
            final_transcript = final_transcript.replace(delimiter, delimiter + '|')
        
        sentences = [s.strip() for s in final_transcript.split('|') if s.strip()]
        
        for i, sentence in enumerate(sentences):
            if sentence:
                if not sentence.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                    sentence += 'ã€‚'
                
                yield send_sse_data('chunk', 
                                  text=sentence,
                                  progress=95 + (i / len(sentences)) * 5)
                await asyncio.sleep(0.05)
        
        # 11. å®Œæˆ
        yield send_sse_data('complete', 
                          progress=100, 
                          message=f'è½‰æ›å®Œæˆï¼Œå…± {len(final_transcript)} å­—',
                          stats={
                              'total_chunks': len(chunks),
                              'successful_chunks': successful_chunks,
                              'total_characters': len(final_transcript),
                              'total_sentences': len(sentences)
                          })
        
    except Exception as e:
        logger.error(f"éŸ³é »è™•ç†å¤±æ•—: {str(e)}", exc_info=True)
        yield send_sse_data('error', 
                          error=f'éŸ³é »è™•ç†å¤±æ•—: {str(e)}',
                          error_type=type(e).__name__)
    
    finally:
        # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æ–‡ä»¶
        if temp_files:
            yield send_sse_data('progress', progress=100, message='æ¸…ç†è‡¨æ™‚æ–‡ä»¶...')
            
            for temp_file in temp_files:
                try:
                    if temp_file and os.path.exists(temp_file):
                        os.unlink(temp_file)
                        logger.info(f"å·²åˆªé™¤è‡¨æ™‚æ–‡ä»¶: {temp_file}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•— {temp_file}: {e}")

@app.post("/api/transcribe")
async def transcribe_audio(audio: UploadFile = File(...)):
    """éŸ³é »è½‰æ–‡å­— API (æ”¯æŒå¤§æ–‡ä»¶åˆ†æ®µè™•ç†) - ä¿®å¾©ç‰ˆ"""
    
    # æª¢æŸ¥æ–‡ä»¶å
    if not audio.filename:
        raise HTTPException(status_code=400, detail="æ²’æœ‰æä¾›æª”æ¡ˆåç¨±")
    
    # æª¢æŸ¥æ–‡ä»¶æ ¼å¼
    file_extension = audio.filename.split('.')[-1].lower()
    if file_extension not in SUPPORTED_FORMATS:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚æ”¯æ´æ ¼å¼: {', '.join(SUPPORTED_FORMATS)}"
        )
    
    # è®€å–æ–‡ä»¶å…§å®¹ä¸¦æª¢æŸ¥å¤§å°
    content = await audio.read()
    file_size = len(content)
    
    # è¨˜éŒ„æ–‡ä»¶ä¿¡æ¯ (åœ¨ file_size å®šç¾©ä¹‹å¾Œ)
    logger.info(f"ğŸ“ æ”¶åˆ°éŸ³é »æ–‡ä»¶: {audio.filename}, å¤§å°: {file_size / 1024 / 1024:.2f}MB")
    
    # æª¢æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
    if file_size > MAX_FILE_SIZE:
        logger.warning(f"âŒ æ–‡ä»¶éå¤§: {file_size / 1024 / 1024:.2f}MB > {MAX_FILE_SIZE / 1024 / 1024}MB")
        raise HTTPException(
            status_code=413,
            detail=f"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ ({MAX_FILE_SIZE // (1024*1024)}MB)"
        )
    
    if file_size == 0:
        logger.error("âŒ æ–‡ä»¶ç‚ºç©º")
        raise HTTPException(status_code=400, detail="ä¸Šå‚³çš„æ–‡ä»¶ç‚ºç©º")
    
    # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
    with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_extension}") as tmp_file:
        tmp_file.write(content)
        temp_file_path = tmp_file.name
    
    logger.info(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {temp_file_path}")
    
    async def generate_response():
        try:
            # ç™¼é€åˆå§‹ç‹€æ…‹
            yield send_sse_data('progress', 
                              progress=0, 
                              message=f'é–‹å§‹è™•ç†éŸ³é »æ–‡ä»¶ ({file_size / 1024 / 1024:.1f}MB)...')
            
            # è™•ç†éŸ³é »æ–‡ä»¶
            async for chunk in process_large_audio(temp_file_path):
                yield chunk
                
        except Exception as e:
            logger.error(f"âŒ è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            yield send_sse_data('error', error=f'è™•ç†å¤±æ•—: {str(e)}')
            
        finally:
            # æ¸…ç†åŸå§‹ä¸Šå‚³æ–‡ä»¶
            try:
                if os.path.exists(temp_file_path):
                    os.unlink(temp_file_path)
                    logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†è‡¨æ™‚æ–‡ä»¶: {temp_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ä¸Šå‚³æ–‡ä»¶å¤±æ•—: {e}")
    
    # è¿”å› SSE éŸ¿æ‡‰
    return StreamingResponse(
        generate_response(),
        media_type='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Content-Type': 'text/event-stream; charset=utf-8',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Cache-Control',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
            'X-Accel-Buffering': 'no'  # ç¦ç”¨åå‘ä»£ç†ç·©è¡
        }
    )


# ä¿æŒå…¶ä»– API ç«¯é» (è¨˜éŒ„ç”Ÿæˆã€è™•é‡è¨ˆç•«ç­‰)
async def generate_report_streaming(transcript: str, social_worker_notes: str, 
                                  selected_sections: List[str], required_sections: List[str]):
    """ç”Ÿæˆè¨˜éŒ„ (ä¿æŒåŸæœ‰é‚è¼¯)"""
    try:
        yield send_sse_data('progress', progress=10, message='æº–å‚™ç”Ÿæˆè¨˜éŒ„...')
        
        # ç°¡åŒ–çš„ prompt (ä½ å¯ä»¥æ ¹æ“šéœ€è¦ä¿®æ”¹)
        prompt = f"""
        è«‹æ ¹æ“šä»¥ä¸‹è¨ªè«‡é€å­—ç¨¿ç”Ÿæˆå°ˆæ¥­çš„ç¤¾å·¥è¨˜éŒ„ï¼š
        
        é€å­—ç¨¿ï¼š
        {transcript}
        
        {'ç¤¾å·¥è£œå……èªªæ˜ï¼š' + social_worker_notes if social_worker_notes else ''}
        
        è«‹ç”Ÿæˆçµæ§‹åŒ–çš„ç¤¾å·¥è¨˜éŒ„ï¼ŒåŒ…æ‹¬å€‹æ¡ˆåŸºæœ¬æƒ…æ³ã€å•é¡Œåˆ†æã€éœ€æ±‚è©•ä¼°ç­‰ã€‚
        """
        
        yield send_sse_data('progress', progress=20, message='æ­£åœ¨ç”Ÿæˆè¨˜éŒ„...')
        
        # èª¿ç”¨ Claude API
        with claude_client.messages.stream(
            model="claude-3-sonnet-20240229",
            max_tokens=4000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        ) as stream:
            
            current_progress = 30
            for event in stream:
                if event.type == "content_block_delta":
                    text_chunk = event.delta.text
                    yield send_sse_data('chunk', text=text_chunk, progress=min(95, current_progress))
                    current_progress += 0.5
                elif event.type == "message_stop":
                    yield send_sse_data('complete', progress=100, message='è¨˜éŒ„ç”Ÿæˆå®Œæˆ')
                    break
                    
    except Exception as e:
        logger.error(f"è¨˜éŒ„ç”Ÿæˆå¤±æ•—: {str(e)}")
        yield send_sse_data('error', error=f'è¨˜éŒ„ç”Ÿæˆå¤±æ•—: {str(e)}')

@app.post("/api/generate-report")
async def generate_report(request: Request):
    """ç”Ÿæˆè¨˜éŒ„åˆç¨¿ API"""
    try:
        data = await request.json()
        
        transcript = data.get('transcript', '').strip()
        if not transcript:
            raise HTTPException(status_code=400, detail="é€å­—ç¨¿å…§å®¹ä¸èƒ½ç‚ºç©º")
        
        social_worker_notes = data.get('socialWorkerNotes', '').strip()
        selected_sections = data.get('selectedSections', [])
        required_sections = data.get('requiredSections', [])
        
        async def generate_response():
            async for chunk in generate_report_streaming(transcript, social_worker_notes, selected_sections, required_sections):
                yield chunk
        
        return StreamingResponse(
            generate_response(),
            media_type='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'Access-Control-Allow-Origin': '*',
            }
        )
        
    except Exception as e:
        logger.error(f"API è™•ç†éŒ¯èª¤: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨éŒ¯èª¤: {str(e)}")

@app.get("/api/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    return {
        'status': 'healthy',
        'service': 'social-work-report-generator-fastapi',
        'timestamp': time.time(),
        'version': '2.0.0',
        'apis': ['transcribe', 'generate-report', 'generate-treatment-plan'],
        'features': ['large_file_support', 'audio_compression', 'concurrent_processing']
    }

@app.get("/api/supported-formats")
async def get_supported_formats():
    """ç²å–æ”¯æ´çš„æ ¼å¼å’Œé…ç½®"""
    return {
        'supported_formats': list(SUPPORTED_FORMATS),
        'max_file_size_mb': MAX_FILE_SIZE // (1024*1024),
        'max_chunk_size_mb': MAX_CHUNK_SIZE // (1024*1024),
        'max_concurrent_transcriptions': MAX_CONCURRENT_TRANSCRIPTIONS
    }

if __name__ == "__main__":
    import uvicorn
    
    logger.info("ğŸš€ å•Ÿå‹• FastAPI ç¤¾å·¥å ±å‘Šç”Ÿæˆæœå‹™...")
    logger.info(f"âœ… OpenAI API é‡‘é‘°å·²è¼‰å…¥")
    logger.info(f"âœ… Claude API é‡‘é‘°å·²è¼‰å…¥")
    
    # ä¿®æ­£çš„å•Ÿå‹•æ–¹å¼
    uvicorn.run(
        "app_fastapi:app",  # ä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼
        host="0.0.0.0", 
        port=5174,
        reload=True,        # ç¾åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨ reload
        log_level="info",
        access_log=True     # é¡¯ç¤ºè¨ªå•æ—¥èªŒ
    )